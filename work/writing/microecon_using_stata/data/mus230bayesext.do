* mus230bayesext.do  for Stata 17

capture log close

********** OVERVIEW OF mus230bayesext.do **********

* Stata program 
* copyright C 2022 by A. Colin Cameron and Pravin K. Trivedi 
* used for "Microeconometrics Using Stata, Second Edition" 
* by A. Colin Cameron and Pravin K. Trivedi (2022)
* Stata Press

* Chapter 30: BAYESIAN METHODS: MARKOV CHAIN MONTE CARLO ALGORITHMS
* 30.2 USER-PROVIDED LOG LIKELIHOOD
* 30.3 MH ALGORITHM IN MATA
* 30.4 DATA AUGMENTATION AND THE GIBBS SAMPLER IN MATA
* 30.5 MULTIPLE IMPUTATION
* 30.6 MULTIPLE-IMPUTATION EXAMPLE

* To run you need no data files
*   in your directory
* Data are generated by program

* No stata community-contributed commands are used

********** SETUP **********

clear all
set linesize 82
set scheme s1mono  /* Graphics scheme */

********** DATA DESCRIPTION **********

* Only simulated data used

* Stata program based on Koop's MATLAB program chapter9b.m
* Probit example of Koop (2003) chapter 9.3 "Bayesian Econometrics"

********** 30.2: ANALYSIS IN STATA 

* Generate artificial dataset for probit illustration 
*  - explanatory variable x ~ N[0,1]
*  - dependent variable   y = 1(0.5*x + e > 0) 
*                     for e ~ N[0,1]

* Generate data N = 100  Pr[y=1|x] = PHI(0.5 + 1.0*x) and x ~ N(0,1)
set obs 100
set seed 1234567
generate x = rnormal(0,1)
generate ystar = 0.5 + 1*x + rnormal(0,1)
generate y = (ystar > 0)
generate cons = 1   // Mata code below requires a regressor for the intercept
summarize
save mus230bayesgenerated, replace

* Fit probit model by MLE
probit y x, nolog

* Fit probit model by command bayesmh
bayesmh y x, likelihood(probit) prior({y:_cons x}, flat) rseed(10101)

* Define evaluator function for probit model for input to command bayesmh
program probitll
    version 17
    args lnf xb
    tempvar lnfj
    qui generate double `lnfj' = ln(normal( `xb')) if $MH_y == 1 
    qui replace `lnfj' = ln(normal(-`xb')) if $MH_y == 0
    qui summarize `lnfj', meanonly
    scalar `lnf' = r(sum)
end

* Fit probit model by command bayesmh with user-provided evaluator
bayesmh y x, llevaluator(probitll) prior({y:_cons x}, flat) rseed(10101)

*********** 30.3: METROPOLIS-HASTING ALGORITHM IN MATA

* Start the Bayesian MCMC using random walk MH

* Define globals for number of reps and the key tuning parameter
global s1 10000      // Number of retained reps
global s0 10000      // Number of burnin reps
global sdscale 0.25  // May need to change c in proposal b + $sdscale*N(0,I)

capture drop beta* 
capture drop accept

* Mata to obtain the posterior draws of b for probit MH algorithm
set seed 10101
mata
    // (1) Create y vector and X matrix from Stata dataset using st_view()
    st_view(y=., ., "y")            // Dependent
    st_view(X=., ., ("cons", "x"))  // Regressors
    Xnames = ("cons", "x")          // Used to label output
    // Calculate a few quantities outside the loop for later use
    n = rows(X)
    k = cols(X)
    ones = J(n,1,1)
    // Specify the number of replications 
    s0 = $s0     // Number of burnin reps
    s1 = $s1     // Number of retained reps
    s = s0+s1    // Total reps
    // Store all draws and MH acceptance rate in the following matrices
    b_all = J(k,s1,0)
    accept_all = J(1,s1,0) 
    // Initialization
    bdraw = J(k,1,0)      // Starting b value is vector of zeros
    lpostdraw = -1*10^10  // Starting value of ln(posterior) is small 
                          // so accept initial MH draw
  
    // (2) Now do MH loop and make the posterior draws
    // Begin MH loop
    for (irep=1; irep<=s; irep++) {
        // Draw new candidate value of b from MH random-walk chain
        bcandidate = bdraw + $sdscale*rnormal(k,1,0,1)
        // Note: For different data, you may need to change the global sdscale
        // Best is bcandidate = bdraw + z, where z ~ N(0, post. variance of b)
        // Compute the log-posterior at the candidate value of b
        // The assumed prior for b is uninformative 
        // so the posterior is proportional to the usual probit likelihood
     probitprob = normal(X*bcandidate)
     lpostcandidate = ones'(y:*ln(probitprob) +(ones-y):*ln(ones-probitprob))
     // Accept the candidate draw on basis of posterior probability ratio 
     // if  uniform > (posterior(bcandidate) / posterior(bdraw))
     // where bcandidate is current b  and  bdraw is previous b
     // Taking logs the rule is the same as
     // if  ln(uniform) > (lpostcandidate - lpostdraw)
     laccprobability = lpostcandidate - lpostdraw
     accept = 0
     if ( ln(runiform(1,1)) < laccprobability ) {
         lpostdraw = lpostcandidate
         bdraw = bcandidate    
         accept = 1 
     }
     // Store the draws after burn-in of b and whether accept draw 
     if (irep>s0) {
         // After discarding burn-in, store all draws
         j = irep-s0
         b_all[.,j] = bdraw         // These are the posterior draws
         accept_all[.,j] = accept   // These are one if new draw accepted
     } 
    }                
    // End MH loop 
  
    // (3) Pass results back to Stata
    // The next command is needed for conformability. It requires $s1 > N
    stata("set obs $s1")
    accept = accept_all'
    st_addvar("byte", "accept")
    st_store(., "accept", accept) 
    beta = b_all'
    // Loop sends each column of beta to Stata as separate variable beta_i 
    for (i=1; i<=k; i++) { 
       v = beta[.,i]
       vname = "beta" + strofreal(i)
       st_addvar("double", vname)
       st_store(., vname, v) 
    }
end

* Simpler way to pass the beta's is 
*   in Mata  drop the command stata("set obs $s1")
*   in Mata  use st_matrix("beta", beta) and drop several lines
*   then in Stata svmat(beta) 
* But this requires Stata/MP or Stata/SE as the Stata matrices are large 

* Analyze the posterior draws from probit MH algorithm 
summarize beta* accept
centile beta2, centile(2.5, 97.5)

* Compute the efficiency of the MH algorithm
generate s = _n
tsset s
qui ac beta2, lags(100) gen(ac)
qui generate ac_sq = ac^2
qui summarize ac_sq
di "Efficiency = " 1/(1+2*r(sum))

* Plot various diagnostics for the posterior draws of b2
qui ac beta2, title("Autocorrelations") lags(100)        ///
    note(" ", ring(0) pos(3)) saving(graph1.gph, replace)
qui line beta2 s if s < 100, title("Trace: First 100 draws") ///
    saving(graph2.gph, replace)
qui line beta2 s, title("Trace") saving(graph3.gph, replace)
qui graph twoway (kdensity beta2) (kdensity beta2 if s<=5000)             ///
    (kdensity beta2 if s>5000), title("Density: All, 1st half, 2nd half") ///
    legend(off) note(" ", ring(0) pos(3)) saving(graph4.gph, replace)

graph combine graph1.gph graph2.gph graph3.gph graph4.gph, ///
    iscale(0.7) ysize(5) xsize(6) rows(2)

qui kdensity beta2, title("Density") note(" ", ring(0) pos(3))  ///
    saving(graph4.gph, replace)
   
*********** 30.4: DATA AUGMENTATION AND THE GIBBS SAMPLER IN MATA

* Define globals for number of reps and the key tuning parameter
use mus230bayesgenerated, clear
global s1 10000      // Number of retained reps
global s0 10000      // Number of burnin reps

* Mata for Gibbs sampler and data augmentation for probit model
set seed 10101
mata
    // (1) Create y vector and X matrix from Stata dataset using st_view
    st_view(y=., ., "y")            // Dependent
    st_view(X=., ., ("cons", "x"))  // Regressors
    Xnames = ("cons", "x")          // Used to label output
    // Calculate a few quantities outside the loop for later use
    n = rows(X)
    k = cols(X)
    Xsquare = cross(X,X)
    Xtxinv = invsym(Xsquare)
    Xtxinvchol = cholesky(Xtxinv)
    v1 = n
    // Specify the number of replications 
    s0 = $s0    // Number of burnin reps
    s1 = $s1    // Number of retained reps
    s = s0+s1   // Total reps
    // Store all draws in the following matrices
    y1 = J(s0+s1,1,0)
    b_all = J(k,s1,0)
    // Prior for beat is noninformative
    // Choose a starting value for latent data
    ystar = y 

    // (2) Now, do Gibbs sampler loop and make the posterior draws
    for (irep=1; irep<=s; irep++) {
        // Posterior step:  draw from beta | y* ~ N[bols*, (X'X)^-1]
        // This is using noninformative prior for beta
        bols = Xtxinv*cross(X,ystar)
        b1 = bols
        bdraw = b1 + Xtxinvchol*rnormal(k,1,0,1)  //invnormal(uniform(k,1))
        // Imputation step: make one draw of vector ystar 
        // where for ith observation ystar_i | y,b  is truncated normal
        // Right: If y = 1, we need draw from truncated N[0,1] with ystar > -mu
        // Left:  If y = 0, we need draw from truncated N[0,1] with ystar < -mu
        for (i=1; i<=n; i++) {
        mu = X[i,.]*bdraw
        if (y[i,1]==0) {
             uright = normal(-mu)*uniform(1,1)        
             ystar[i,1] = mu + invnormal(uright)
        }
        else {
            uleft = normal(-mu) + (1-normal(-mu))*uniform(1,1)
            ystar[i,1] = mu + invnormal(uleft)
        }
     }
     // Store the draws of b after burn-in plus a diagnostic used in Koop
     if (irep>s0) {
         // After discarding burn-in, store all draws
         j = irep-s0
         b_all[.,j] = bdraw    // These are the posterior draws
    } 
    }                 
    // End Gibbs loop 
  
    // (3) Pass results back to Stata
    // The next command is needed for conformability. It requires $s1 > N
    stata("set obs $s1")
    beta = b_all'
    // Loop sends each column of beta to Stata as separate variable beta_i 
    for (i=1; i<=k; i++) { 
        v = beta[.,i]
        vname = "beta" + strofreal(i)
        st_addvar("double", vname)
       st_store(., vname, v) 
    }
end

/*  
  beta = b_all'
  st_addvar("float", ("beta1", "beta2"))
  // The following line is needed for conformability 
  // It assumes that s1 (number of draws) exceeds the original sample size 
  stata("set obs $s1")
  st_store(., ("beta1", "beta2"), beta)
*/

* Analyze the posterior draws from probit Gibbs sampler algorithm 
summarize beta* 
centile beta2, centile(2.5, 97.5)

* Compute the efficiency of the Gibbs sampler algorithm
generate s= _n
tsset s
qui ac beta2, lags(100) gen(ac)
qui generate ac_sq = ac^2
qui summarize ac_sq
di "Efficiency = " 1/(1+2*r(sum))

* Plot various diagnostics for the posterior draws of b2
qui ac beta2, title("Autocorrelations") lags(100) ///
    note(" ", ring(0) pos(3)) saving(graph1, replace)
qui line beta2 s if s < 100, title("Trace: First 100 draws") ///
    saving(graph2, replace)
qui line beta2 s, title("Trace") saving(graph3, replace)
qui graph twoway (kdensity beta2) (kdensity beta2 if s<=5000)             ///
    (kdensity beta2 if s>5000), title("Density: All, 1st half, 2nd half") ///
     legend(off) note(" ", ring(0) pos(3)) saving(graph4, replace)

graph combine graph1.gph graph2.gph graph3.gph graph4.gph, ///
    iscale(0.7) ysize(5) xsize(6) rows(2)

*********** 30.5: MULTIPLE IMPUTATION

* Create complete data with regressors x2 and x3 correlated  
clear
set seed 10101
matrix C = (1, .5 \ .5, 1)
drawnorm x2 x3, n(10000) corr(C)
generate y = 0 + 1*x2 + 2*x3 + rnormal(0,1)
summarize

* OLS on complete data
regress y x2 x3

* Create incomplete data by dropping some x2 (MAR)
set seed 12345
replace x2 = . if x3 > rnormal(0.5, 0.5)
summarize
qui save incomplete, replace

* OLS on incomplete data
regress y x2 x3

* Summarize missingness
misstable summarize
misstable patterns

* Declare dataset type (long), register imputed variables, perform 3 imputations
mi set mlong
mi register imputed x2
mi register regular y x3
mi impute mvn x2 = y x3, add(3) rseed(10101) burnin(100) burnbetween(100)

* Multiple imputation creates the following data and variables
summarize

* Check reasonableness of imputed x2
mi xeq 0 1 3: summarize y x2 x3

* Plot x2 against x3 for nonmissing data and first imputation
qui graph twoway (scatter x2 x3 if _mi_m==0, msize(tiny))          ///
    (lfit x2 x3 if _mi_m==0, lwidth(thick)), ytitle("x2")          ///
    title("Nonmissing data: x2 versus x3") legend(off) saving(graph1, replace)
qui graph twoway (scatter x2 x3 if _mi_m==1, msize(tiny))          ///
    (lfit x2 x3 if _mi_m==1, lwidth(thick)), ytitle("x2")          ///
    title("Imputed data: x2 versus x3") legend(off) saving(graph2, replace)  
graph combine graph1.gph graph2.gph, ycommon xcommon iscale(1.2)  ///
    ysize(2.5) xsize(6) rows(1) 


* Describe the imputed data
mi describe

* Fit the model
mi estimate, dots: regress y x2 x3

* Create three complete datasets with imputed  
mi convert flongsep midata, replace clear

* Perform OLS estimation on each of these three imputed datasets
use _1_midata, clear
qui regress y x2 x3, noheader
scalar b1x2 = _b[x2]
use _2_midata, clear
qui regress y x2 x3, noheader
qui scalar b2x2 = _b[x2]
use _3_midata, clear
qui regress y x2 x3, noheader
scalar b3x2 = _b[x2]
display "Ave OLS coeff of x2 over 3 imputed samples = " (b1x2 + b2x2 + b3x2)/3

* Impute and estimate with 10 imputations
use incomplete, clear
mi set mlong
mi register imputed x2
mi register regular y x3
qui mi impute mvn x2 = y x3, add(10) rseed(10101) burnin(100) burnbetween(100)
mi estimate, dots: regress y x2 x3

*** Erase datasets created by this program

erase mus230bayesgenerated.dta
erase incomplete.dta
erase graph1.gph
erase graph2.gph
erase graph3.gph
erase graph4.gph
erase midata.dta
erase _1_midata.dta
erase _2_midata.dta
erase _3_midata.dta

********** END
